{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv\n",
    "train_data = []\n",
    "with open('mnist_train.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        train_data.append(\n",
    "            (\n",
    "                int(row[0]),\n",
    "                np.array([int(x) / 255 for x in row[1:]])\n",
    "            )\n",
    "        )\n",
    "test_data = []\n",
    "with open('mnist_test.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        test_data.append(\n",
    "            (\n",
    "                int(row[0]),\n",
    "                np.array([int(x) / 255 for x in row[1:]])\n",
    "            )\n",
    "        )\n",
    "# img_avg = np.mean([x[1] for x in train_data])\n",
    "# img_std = np.std([x[1] for x in train_data])\n",
    "# train_data = [(x[0], (x[1] - img_avg) ) for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:01, 352.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:01, 400.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:01, 430.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:01, 416.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:01, 404.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:01, 392.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [00:00, 332.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mcross_entropy(output, labels)\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/tinkering/learning-deep-learning/neural_net_basics/nn.py:62\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(ordered_nodes):\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tinkering/learning-deep-learning/neural_net_basics/nn.py:126\u001b[0m, in \u001b[0;36mTensor.matmul.<locals>._backprop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad(np\u001b[38;5;241m.\u001b[39mmatmul(out\u001b[38;5;241m.\u001b[39mgrad, other\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# other.grad B x C = (self.T) B x A * (out) A x C\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m other\u001b[38;5;241m.\u001b[39maccumulate_grad(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "model = nn.MLP(28 * 28, 10)\n",
    "optim = nn.SGD(model.parameters(), lr=0.01)\n",
    "i = 0\n",
    "for epoch in range(20):\n",
    "    for labels, imgs in tqdm(nn.DataLoader(train_data, batch_size=BATCH_SIZE)):\n",
    "        i += 1\n",
    "        output = model.forward(nn.Tensor(imgs, name=\"imgs\"))\n",
    "        loss = nn.cross_entropy(output, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    for labels, imgs in nn.DataLoader(test_data, batch_size=len(test_data)):\n",
    "        output = model.forward(nn.Tensor(imgs, name=\"imgs\"))\n",
    "        pred = np.argmax(output.value, axis=1)\n",
    "        correct = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == pred[i]:\n",
    "                correct += 1\n",
    "        print(correct / len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "SGD\n",
    "Momentum\n",
    "Dropout\n",
    "Batchnorm\n",
    "Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
